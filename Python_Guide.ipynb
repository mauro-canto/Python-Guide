{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEP 8 STYLE GUIDE FOR PYTHON CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.python.org/dev/peps/pep-0008/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT REMARK: Add path environment variable in windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If installing anaconda from new, select the option of adding Python to the environment variable.\n",
    "\n",
    "If you already installed Anaconda or Python and is not into path environment variables, you have to add the paths:\n",
    "- C:\\Users\\Mauro\\anaconda3\n",
    "- C:\\Users\\Mauro\\anaconda3\\Scripts\n",
    "- C:\\Users\\Mauro\\anaconda3\\Library\\bin\n",
    "- C:\\Users\\Mauro\\anaconda3\\Library\\usr\\bin\n",
    "\n",
    "Try it launching \"python\" from the command window.\n",
    "\n",
    "If there is a path like this one on the path environment variables: %USERPROFILE%\\AppData\\Local\\Microsoft\\WindowsApps\n",
    "delete it, otherwise it will always redirect you to the Microsoft Store.\n",
    "\n",
    "For more instructions follow the link below:\n",
    "https://medium.com/@hektorprofe/tutorial-windows-10-agregar-el-python-de-anaconda-al-path-para-utilizarlo-en-la-cmd-y-powershell-72acf22901a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT REMARK: SHIFT + TAB = INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that when using a built-in function or method we can press: SHIFT+TAB to open the information window about such function or method, this allows to see the arguments it takes and in which form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CREATION OF ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User guide:  https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create an environment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the Anaconda Prompt type:\n",
    "    conda create --name \"environment_name\" \"initiation_package/library\"\n",
    "    where: \"environment_name\" stands for the environment name.\n",
    "           \"initiation_package/library\" stands for the base package or library you want to initiate your environment with.\n",
    "\n",
    "If you do not specify a version of python the environment will be created with your base version of Python.\n",
    "\n",
    "For example to create an environment named \"test1\" with the library \"numpy\" we would do:\n",
    "    conda create --name test1 numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Activate and deactivate an environment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To activate an environment, use:\n",
    "   activate \"environment_name\"\n",
    "\n",
    "To deactivate an environment, use:\n",
    "   deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Install libraries in an environment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We do it like we always did, but activating the environment first.\n",
    "For example, to install the library \"pandas\" in our previously created environment \"test1\" we would do:\n",
    "   activate test1\n",
    "   conda install pandas\n",
    "   conda update pandas (To update to the latest version)\n",
    "######################################\n",
    "We can also use:\n",
    "    pip install pandas\n",
    "    pip install --upgrade pandas (To update to the latest version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create an environment with different versions of Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you desire to create an environment with another version of Python that is not your base version, the command is:\n",
    "   conda create --name \"environment_name\" python=\"python_version\" \"initiation_package/library\"\n",
    "\n",
    "For example to create an environment in Python 3.5 and with the whole Anaconda distribution (notice it will take furhter\n",
    "time to install) we would type:\n",
    "   conda create --name test2 python=3.5 anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 List and information of the created environments"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " conda info --envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Use Python from the Prompt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To enter Python from the Anaconda prompt or Windows command prompt (if Python is as the path variable, see above) we type:\n",
    "   python\n",
    "\n",
    "To quit and come back to the directory explorer we type:\n",
    "   quit()\n",
    "\n",
    "To run a python script from the Anaconda prompt, we navigate to the folder where our script is and then execute it by:\n",
    "   python filename.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Load a Python script (.py) script in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To load the content of a python script on Jupyter Notebook, we do:\n",
    "   %load filename.py\n",
    "\n",
    "For this to work, both the .ipynb and the .py file need to be in the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MAP, LAMBDA AND FILTER EXPRESSIONS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The point in using python functions is to perform a series of operations several times in our routines. Nevertheless if we only intend to use that operation one time in our script it may not make sense to write an entire function, we can use a lambda expression.\n",
    "\n",
    "The map function is a python built-in function that maps some function to some iterable sequence.\n",
    "\n",
    "These two concepts: lambda expressions and map functions are very powerful when combined.\n",
    "\n",
    "Example:\n",
    "    def two_times(argument):\n",
    "        return argument*2\n",
    "        \n",
    "    seq = [1, 2, 3, 4]\n",
    "    map(two_times, seq)\n",
    "This will return a map object, to see the new list we have to convert it to a list with the command: list().\n",
    "######################################\n",
    "Nevertheless this can be done even shorter with a lambda statement:\n",
    "\n",
    "Example:\n",
    "    list(map(lambda arg:arg*2, seq))\n",
    "    \n",
    "The lamba expression \"lambda arg:arg*2\" defines the previous function \"two_times\" but in a shorter statement.\n",
    "######################################\n",
    "The filter python function works in a similar way than the map function but not exactly the same. It is an iterator returning those items for which the function is true.\n",
    "\n",
    "Example:\n",
    "    We want to know if a number is even or not in a sequence.\n",
    "    \n",
    "    seq = [1, 2, 3, 4, 5]\n",
    "    list(filter(lambda arg:arg%2 == 0, seq))\n",
    "    >>> [2, 4]\n",
    "\n",
    "It executes the lambda expressions which calculates the modulus of the division by 2, and returns only the items for which that expression is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. OBJECT ORIENTED PROGRAMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Definition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Object-oriented programming (OOP) is a method of structuring, a programming paradigm that provides a means of structuring programs by bundling related properties and behaviours into individual objects.\n",
    "\n",
    "OOP models real-world entities as software objects that have some data associated with them and can perform certain functions, for example, an bject could represent a person with properties like name, age and address and behaviours such as walking, talking or running.\n",
    "\n",
    "Another common paradigm is procedural programming, which structures a program like a recipe in the sense that it provides a set of steps, in the form of functions and code blocks, that flow sequentially in order to complete a task.\n",
    "\n",
    "The key concept is that objects are at the center og object-orienteed programming, not only representing the data, as in procedural programming, but in the overall structure of the program as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Primitive data structures like numbers, strings and lists are designed to represent simple pieces of information, like the cost of an item, a numeric constant, a message, etc... But if you want to represent something more complex, store everything in single variables can make larger code files difficult to manage and understand and can induce errors. A great way to deal with that is using classes.\n",
    "\n",
    "Classes are used to create user-defined data structures. Classes define object properties called attributes and functions called methods, which identify the behaviours and actions that an object created from the class can perform with its data.\n",
    "\n",
    "A class is a blueprint for how something should be defined. It does not actually contain any data. While the class is a blueprint, an instance is an object that is built from a class and contains real data. Put another way, a class is like a form of questionnaire while an instance is like a form that has been filled out with information. Just like many people can fill out the same form with their own unique information, many instances can be created from a single class.\n",
    "\n",
    "Python class names are written in CapitalizedWords notation by convention. For example, a class for a specific breed of dog like the Jack Rusell Terrier would be written as \"JackRusselTerrier\".\n",
    "\n",
    "The attributes that all object must have are defined in a method called \"__init__()\". Every time a new object is created, \"__init__()\" sets the initial state of the object by assigning the values of the object's properties, it initializes each new instance of the class.\n",
    "\n",
    "You can give \"__init__()\" any number of parameters, but the first parameter will always be a variable called \"self\". When a class instance is created the instance is automatically passed to the self parameter so that new attributes can be defined on the object.\n",
    "######################################\n",
    "Example:\n",
    "    class Dog:\n",
    "        # Class attribute\n",
    "        species = \"Canis\"\n",
    "        \n",
    "        # Instance attribute\n",
    "        def __init__(self, name, age):\n",
    "            self.name = name\n",
    "            self.age = age\n",
    "######################################\n",
    "\"self.name = name\" and \"self.age = age\" create attributes called \"name\" and \"age\" and assign to them the values of \"name\" and \"age\". \n",
    "\n",
    "The attributes called in \"__init()__\" are called instance attributes. An instance attribute is specific to a particular instance of the class. All Dog objects will hava a name and age attributes but its values may vary depending the dog instance. On the other hand, class attributes are attributes that have the same values for all class instances. You can define a class attribute by assigning a value to a variable name outside \"__init__()\", for example \"species = 'Canis'\". When an instance of the class is created, class attributes are automatically created and assigned to their initial values. Use class attributes to defined properties that should have the same value for every class instance. Use instance attributes for properties that vary from one instance to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Instantiate an Object"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following the same example than before, to instantiate an a Dog object we will need to pass the arguments for the name and age, the argument \"self\" does not need to be passed, this is just for Python to pass to create a new instance and pass to the first parameter, therefore it needs to be added when defining a class not when instantiating it.\n",
    "######################################\n",
    "Example:\n",
    "    dog1 = Dog(\"Buddy\", 9)\n",
    "    \n",
    "    print(dog1.name)\n",
    "    >>> 'Buddy'\n",
    "    print(dog1.age)\n",
    "    >>> 9\n",
    "    print(dog1.species)\n",
    "    >>> 'Canis'\n",
    "    \n",
    "One of the biggest advantages of using classes to organize data is that instances are guaranteed to have the attributes you expect are they are represented in a standarized and commonly defined way. Although the attributes are guaranteed to exist, their values can be changed dynamically.\n",
    "######################################\n",
    "Example:\n",
    "    dog1.age = 12\n",
    "    print(dog1.age)\n",
    "    >>> 12\n",
    "    dog1.species = \"Canis lupus\"\n",
    "    print(dog1.species)\n",
    "    >>> 'Canis lupus'\n",
    "######################################\n",
    "The key takeaway here is that custom objects are mutable by default. An object is mutable if it can be altered dynamically. Variables, lists and dictionaries are mutable, but strings and tuples are immutable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Methods are functions that are defined inside a class and can only be called from an instance of that class. Just like \"__init__()\", the first parameter of a method is always \"self\".\n",
    "######################################\n",
    "Example:\n",
    "    class Dog:\n",
    "        # Class attribute\n",
    "        species = \"Canis\"\n",
    "        \n",
    "        # Instance attribute\n",
    "        def __init__(self, name, age):\n",
    "            self.name = name\n",
    "            self.age = age\n",
    "            \n",
    "        # Instance method\n",
    "        def description(self):\n",
    "            return f\"{self.name} is {self.age} years old.\n",
    "            \n",
    "        # Another instance method\n",
    "        def speak(self, sound):\n",
    "            print(\"{} says {}\".format(self.name, sound))\n",
    "######################################\n",
    "Example:\n",
    "    dog1 = Dog(\"Miles\", 4)\n",
    "    dog1.description()\n",
    "    >>> 'Miles is 4 years old'\n",
    "    dog1.speak(\"Woof Woof\")\n",
    "    >>> 'Miles says Woof Woof'\n",
    "######################################\n",
    "\n",
    "When writing your own classes, it is a good idea to have a method that returns the description and useful information about the classes. Nevertheless creating a method called \"description\" is not the most Pythonic way of doing that.\n",
    "\n",
    "When you try to print an instance you will get that:\n",
    "    print(dog1)\n",
    "    >>> <__main__.Dog object at 0x00aef70>\n",
    "    \n",
    "This message tells you that \"dog1\" is an object stored at a certain memory space, nevertheless this information is not very helpful from a user point of view to describe the class and provide information. Instead, we can change what gets printed by defining a special instance method called \"__str__()\".\n",
    "######################################\n",
    "Example:\n",
    "    class Dog:\n",
    "        # Class attribute\n",
    "        species = \"Canis\"\n",
    "        \n",
    "        # Instance attribute\n",
    "        def __init__(self, name, age):\n",
    "            self.name = name\n",
    "            self.age = age\n",
    "            \n",
    "        # Instance method\n",
    "        def __str__(self):\n",
    "            return f\"{self.name} is {self.age} years old.\n",
    "            \n",
    "        # Another instance method\n",
    "        def speak(self, sound):\n",
    "            print(\"{} says {}\".format(self.name, sound))\n",
    "######################################\n",
    "If now we do:\n",
    "    print(dog1)\n",
    "    >>> >>> 'Miles is 4 years old'\n",
    "\n",
    "The methods that begin with double underscores are called dunder methods and there are many we can use to customize our classes in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Inheritance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Inheritance is the process by which one class takes on the attributes and methods of another. Newly formed classes are called child classes and the classes they are derived from are called parent classes.\n",
    "\n",
    "Child classes can override or extend the attributes and methods of parent classes. In other words, child classes inherit all of the parent's attributes and methods but can also specify attributes and methods that are unique to themselves.\n",
    "\n",
    "######################################\n",
    "Example: Imagine that you are at a dog park with many dogs of different breed and you want to model the park with Python classes.\n",
    "    class Dog:\n",
    "        species = \"Canis\"\n",
    "        \n",
    "        def __init__(self, name, age, breed):\n",
    "            self.name = name\n",
    "            self.age = age\n",
    "            self.breed = breed\n",
    "            \n",
    "        def speak(self, sound):\n",
    "            return f\"{self.name} says {sound}\"\n",
    "            \n",
    "Now you can model the park:\n",
    "    miles = Dog(\"Miles\", 4, \"Jack Rusell Terrier\")\n",
    "    buddy = Dog(\"Buddy\", 9, \"Dachsund\")\n",
    "    jack = Dog(\"Jack\", 3, \"Bulldog\")\n",
    "######################################\n",
    "Now imagine that each breed of dog have a different bark: Jack rusell terrier do \"Arf\", dachsunds do \"Yap\" and bulldogs do \"Woof\". Using the Dog class we must supply a string for the bark sound everytime we call the \".speak()\" the method, this is repetitive and inconvenient. Moreover, the string representing the sound that each Dog instance makes should be determined by the \".breed\" attribute without having to manually pass the correct string.\n",
    "\n",
    "We can simplify this by creating a child class for each breed of dog. This allows to extend the functionality that each child class inherits.\n",
    "\n",
    "To create a child class, you create a new class with its own name and then put the name of the parent class in parentheses as an argument of the child class.\n",
    "\n",
    "Since different breeds of dogs have different barks, you want to provide a default value for the sound argument of their respective \".speak()\" method. To do this , you need to override \".speak()\" in the class definition for each breed. To override a method defined on the parent class, you define a method with the same name on the child class.\n",
    "######################################\n",
    "Example:\n",
    "    class Dog:\n",
    "        species = \"Canis\"\n",
    "        \n",
    "        def __init__(self, name, age):\n",
    "            self.name = name\n",
    "            self.age = age\n",
    "        \n",
    "        def speak(self, sound):\n",
    "            return f\"{self.name} says {sound}\"\n",
    "    \n",
    "    class JackRussellTerrier(Dog):\n",
    "        def speak(self, sound = \"Arf\"):\n",
    "            return f\"{self.name} says {sound}\"\n",
    "    \n",
    "    class Dachsund(Dog):\n",
    "        def speak(self, sound = \"Yap\"):\n",
    "            return f\"{self.name} says {sound}\"\n",
    "            \n",
    "    class Bulldog(Dog):\n",
    "        def speak(self, sound = \"Woof\"):\n",
    "            return f\"{self.name} says {sound}\"\n",
    "\n",
    "    miles = JackRussellTerrier(\"Miles\", 4)\n",
    "    buddy = Dachsund(\"Buddy\", 9)\n",
    "    jack = Bulldog(\"Jack\", 3)\n",
    "    \n",
    "    miles.speak()\n",
    "    >>> 'Miles says Arf'\n",
    "######################################\n",
    "Sometimes dogs make different barks, so if Miles gets angry and growls, you can still call \".speak()\".\n",
    "######################################\n",
    "Example:\n",
    "    miles.speak(\"Grrr\")\n",
    "    >>> 'Miles says Grrr'\n",
    "######################################\n",
    "To determine which class a given object belongs to, you can use the built-in function \"type()\". Also, if you want to determine if an object is instance of a specific class you can use the built-in function \"is-instance()\".\n",
    "######################################\n",
    "Example:\n",
    "    type(miles)\n",
    "    >>> <class '__main__.JackRussellTerrier'>\n",
    "    isinstance(miles, Dog)\n",
    "    >>> True\n",
    "    isinstance(miles, Bulldog)\n",
    "    >>> False\n",
    "######################################\n",
    "\n",
    "Another thing to keep in mind about class inheritance is that changes to the parent class automatically propagate to child classes, AS LONG AS THE ATTRIBUTE OR METHOD CHANGED IS NOT OVERRIDDEN IN THE CHILD CLASS, THEN IS THE OVERRIDEN CHILD ATTRIBUTE THE ONE THAT PREVAILS.\n",
    "######################################\n",
    "Example:\n",
    "     class Dog:\n",
    "        species = \"Canis\"\n",
    "        \n",
    "        def __init__(self, name, age):\n",
    "            self.name = name\n",
    "            self.age = age\n",
    "        \n",
    "        def speak(self, sound):\n",
    "            return f\"{self.name} BARKS {sound}\"\n",
    "            \n",
    "     class JackRussellTerrier(Dog):\n",
    "        def speak(self, sound = \"Arf\"):\n",
    "            return f\"{self.name} SAYS {sound}\"\n",
    "            \n",
    "    miles = JackRussellTerrier(\"Miles\", 4)\n",
    "    buddy = Dog(\"Buddy\", 9)\n",
    "    \n",
    "    buddy.speak(\"Yap\")\n",
    "    >>> 'Buddy BARKS Yap'\n",
    "    miles.speak()\n",
    "    >>> 'Miles SAYS Arf'\n",
    "    miles.speak(\"Woof\")\n",
    "    >>> 'Miles SAYS Woof'\n",
    "######################################\n",
    "Observe how any change that we make in the method \".speak()\" will affect \"buddy\" but will not affect \"miles\" as Miles is a \"JackRussellTerrier\" child instance of dog where the \".speak()\" has been overridden.\n",
    "\n",
    "Sometimes it makes sense to completely override a method from a parent class. But in this instance, we don not want the \"JackRussellTerrier\" class to lose any changes that might be made to the formatting of the output string of the \"Dog.speak()\" method.\n",
    "\n",
    "To do this, you still need to define a \".speak()\" method on the child \"JackRussellTerrier\" class, but instead of explicitly defining the output string, you can call the parent method \"Dog.speak()\" inside the child class. You can access the parent class from inside a method of a child class by using \"super()\".\n",
    "######################################\n",
    "Example:\n",
    "     class Dog:\n",
    "          species = \"Canis\"\n",
    "        \n",
    "          def __init__(self, name, age):\n",
    "              self.name = name\n",
    "              self.age = age\n",
    "        \n",
    "          def speak(self, sound):\n",
    "              return f\"{self.name} says {sound}\"\n",
    "            \n",
    "      class JackRussellTerrier(Dog):\n",
    "          def speak(self, sound = \"Arf\")\n",
    "              return super().speak(sound)\n",
    "######################################\n",
    "When you call \"super().speak(sound)\" inside \"JackRussellTerrier\", Python searches the parent class, Dog, for a \".speak()\" method and calls it inside the child class with the variable \"sound\".\n",
    "\n",
    "NOTE: Observe that this examples of class hierarchy are very straightforward, in other real world examples, this can get quite complicated. \"super()\" does much more than just serch the parent class for a method or an attribute. It traberses the entire class hierarchy for a matching method or attribute. If you aren't careful, super cannot have the expected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NUMPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation User Guide: https://numpy.org/doc/1.20/user/index.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NumPy is an outide library that stands for \"Numerical Python\". Is one of the essential libraries in Python.\n",
    "\n",
    "It is a nuerical library for Python that allows for extremely fast data generation and handling. It uses arrays\n",
    "which efficiently store data much better than a built-in Python list.\n",
    "\n",
    "Generally is import as np:\n",
    "   import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Arrays"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An array is entered like a nested list, but with the NumPy library it is assigned matricial properties:\n",
    "   np.array([[row1], [row2], [row3]])\n",
    "\n",
    "Example:\n",
    "   np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "This will produce an array like:\n",
    "  | 1 2 3 |\n",
    "  | 4 5 6 |\n",
    "  | 7 8 9 |\n",
    "######################################\n",
    "   np.arange(start, stop, step)\n",
    "\n",
    "This will produce an array of numbers starting at \"start\" and ending at \"stop\" spaced by \"step\". This is analogous at the\n",
    "command \"range(start, stop, step)\" the difference is that with \"range\" we will be creating a list while with \"np.array\" we \n",
    "will be creating an array that is susceptible to all the transformations and operations able to be performed by the Numpy\n",
    "library.\n",
    "######################################\n",
    "   np.zeros((m, n))\n",
    "   np.ones((m, n))\n",
    "\n",
    "It creates an array of dimensions mxn of zeros or ones.\n",
    "######################################\n",
    "   np.linspace(start, stop, num_divisions)\n",
    "\n",
    "It returns \"num_divisions\" evenly spaced number over the specified interval of \"start\" and \"stop\". it works different than\n",
    "\"np.arange(start, stop, step)\", because in \"arange\" you specify the step between the numbers and in \"linspace\" you specify\n",
    "the number of evenly spaced divisions between start and stop.\n",
    "######################################\n",
    "   np.eye(n)\n",
    "\n",
    "It creates an nxn identity matrix, we only need one input as the identity matrix is a square matrix by definition.\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random sampling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We may want to create random arrays containing data which follows a given statistical distribution.\n",
    "\n",
    "   np.random.rand(m, n)\n",
    "\n",
    "It creates an array with a nxm shape and populates it with random samples from a uniform distribution over [0, 1). A uniform\n",
    "distribution means that all numbers bewteen 0 and 1 have the same probability of being picked.\n",
    "######################################\n",
    "   np.random.randn(m, n)\n",
    "\n",
    "If what we want is a standard normal Gaussian distribution with mean = 0 and standard deviation = 1.\n",
    "######################################\n",
    "   np.random.randint(low, high, size)\n",
    "\n",
    "It will pick a set of random integers of size \"size\" between \"low\" (inclusive) and \"high\" (exclusive).\n",
    "######################################\n",
    "We can configure the random sampling to obtain the same by starting the random seed always at the same value by doing:\n",
    "   np.randomseed(x)\n",
    "Where \"x\" is a number indicating the seed starting position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Reshaping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given a set of numbers we can reshape them in a new format with the method \".reshape(m, n)\".\n",
    "\n",
    "Example:\n",
    "We can reshape a unidimensional array of 25 numbers into a 5x5 matrix.\n",
    "   array = np.arange(25)\n",
    "   array.reshape(5,5)\n",
    "######################################\n",
    "The attribute \"array.shape\" returns the shape of the array.\n",
    "######################################\n",
    "The attribute \"array.dtype\" shows what type of variables does the array contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Maximum and minimum values and its positions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given an array, we can locate the maximum and minimum values and its index position inside the data set.\n",
    "\n",
    "The attribute \"array.max()\" returns the maximum value of the array.\n",
    "######################################\n",
    "The attribute \"array.argmax()\" returns the index where the maximum value is located.\n",
    "######################################\n",
    "The same logic is applied to the attributes \"array.min()\" and \"array.argmin()\" but in this case for the minimum values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Operations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All the operations done to an array are performed element by element. That means that the operations are applied to every\n",
    "element of the array individually.\n",
    "\n",
    "Some operators are:\n",
    "   np.sqrt(array)\n",
    "   np.exp(array)\n",
    "   np.max(array)\n",
    "   np.min(array)\n",
    "   np.sin(array)\n",
    "   np.cos(array)\n",
    "   np.tan(array)\n",
    "   np.log(array)\n",
    "######################################\n",
    "There are some attributes that allow to some operations be performed:\n",
    "   \"array.sum()\" performs the summation of all the elements of the array.\n",
    "######################################\n",
    "   \"array.mean()\" and \"array.std()\" calculate the mean and standard deviation of all the elements contained in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Matricial operations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Contrary to the operators previouslt mentioned, NumPy allows for matricial and vectorial operators. Remember that a vector \n",
    "will be treated like a matrix with only one row or column.\n",
    "\n",
    "Given two vectors \"A\" and \"B\" defined as:\n",
    "   A = np.array([x1, y1, z1])\n",
    "   B = np.array([x2, y2, z2])\n",
    "Their corresponding dot and cross products can be calculated as:\n",
    "   \"np.dot(A, B)\" for the dot product.\n",
    "   \"np.cross(A, B)\" for the cross product.\n",
    "######################################\n",
    "Given two matrices [C] and [D] the matricial product can be performed as:\n",
    "   \"np.dot(C, D)\" or \"C.dot(D)\" to do [C].[D]\n",
    "   \"np.dot(D, C)\" or \"D.dot(C)\" to do [D].[C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Indexing and broadcasting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Indexing in Numpy works as a normal Python list. What can be done here with arrays that cannot be done with lists is \n",
    "broadcasting. That means selecting a range of values and substitute them by a number.\n",
    "\n",
    "   array[x:y] = z\n",
    "In here, all elements in the positions from \"x\" to \"y\" including both, will be substituted by the number \"z\".\n",
    "######################################\n",
    "When reasigning some subset of an array into a new variable, the data is not copied but substituted in the original array.\n",
    "This is because if not, when working with large datasets while copying and reasigning we may run out of RAM. If we do not\n",
    "want to change the original variable, but to create another one to perform the broadcasting, we can explicitly copy an array\n",
    "into another variable:\n",
    "   \"B = A.copy()\" where \"A\" is an array.\n",
    "######################################\n",
    "To grab an element in a position i, j from a matrix A we do:\n",
    "   A[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Conditional selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Conditional selection occurs when we want to grab some elements of a matrix based on some sort of condition. We compare\n",
    "our array with the desired condition and this will produce an array of booleans. The we pass this array of booleans in our\n",
    "original array and we grab only the \"True\" elements.\n",
    "\n",
    "Example:\n",
    "   array = np.arange(0, 6) ---> [0, 1, 2, 3, 4, 5, 6]\n",
    "   boolean_array = array > 2 ---> [False, False, False, True, True, True]\n",
    "   output_array = array[boolean_array] ---> [3, 4, 5]\n",
    "######################################\n",
    "The previous example can be executed in a shorter way with just a line of code by doing:\n",
    "   output_array = array[array>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. PANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation User Guide: https://pandas.pydata.org/docs/user_guide/index.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Named after \"panel-data\", was created by Wes Mckinney to help with his financial work.\n",
    "\n",
    "Pandas has a fast and efficient DataFrame object for data manipulation with integrated indexing. It also includes tools for reading and writing data between data structures in different formats. Moreover it has gret interaction with visualization libraries as matplotlib.\n",
    "\n",
    "Pandas is imported as:\n",
    "    import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Series"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Series are similar to arrays, except that we can give them a named or datetime index instead of just a numerical index.\n",
    "######################################\n",
    "We can convert some object into series:\n",
    "    pd.Series(data, index)\n",
    "######################################\n",
    "\n",
    "For data types as arrays or list if index is not specified it is automatically set as 0, 1, 2, ... For dictionaries if index is not specified, by default, keys are placed as indexes and values as data.\n",
    "\n",
    "Operations can be performed with series. The operations will be performed index by index if the indexes match, for indexes that do not match, \"NaN\" is produced.\n",
    "######################################\n",
    "Example:\n",
    "    serie1 = pd.Series([1, 2, 3], [\"USA\", \"CHINA\", \"SPAIN\"])\n",
    "    serie2 = pd.Series([1, 2, 3], [\"USA\", \"CHINA\", \"FRANCE\"])\n",
    "    \n",
    "    serie1 + serie2\n",
    "    >>> \"USA\"    2\n",
    "        \"CHINA\"  4\n",
    "        \"SPAIN\"  NaN\n",
    "        \"FRANCE\" Nan\n",
    "######################################\n",
    "For the indexes of \"SPAIN\" and \"FRANCE\" as there is not any index match, the value of the data cannot be added up and \"NaN\" is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 DataFrames: Creation and Conditional Selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can create a DataFrame with the commands:\n",
    "    pd.DataFrame(data, index, columns)\n",
    "######################################    \n",
    "Here we have an \"index\" which will index the rows and \"columns\" which will index the columns. We will have a pandas series that will look like a table. Every column of the table will be like a pandas series, so a DataFrame is just a bunch of series that share the same index.\n",
    "######################################\n",
    "We can access a column of such dataframe by passing an index:\n",
    "    DataFrame[\"col_name\"] or DataFrame[[\"col_name1, col_name2, ..., col_namen]]\n",
    "######################################\n",
    "New columns can be defined as:\n",
    "    DataFrame[\"new\"] = -----\n",
    "######################################\n",
    "To remove columns:\n",
    "    DataFrame.drop(\"index/col\", axis = {0, 1}, inplace = True)\n",
    "\n",
    "\"axis = 0\" for removing rows and \"axis = 1\" for columns. The \"inplace = True\" argument is done like a confirmation, to not accidentally remove data.\n",
    "######################################\n",
    "To select rows (which are also series) we use the commands:\n",
    "    DataFrame.loc[\"index\"]\n",
    "    DataFrame.iloc[\"index position\"]\n",
    "######################################\n",
    "To locate a single value we can access to it as te elements of a matrix\n",
    "    DataFrame.loc[\"row\", \"column\"]\n",
    "    DataFrame.iloc[m, n]\n",
    "######################################\n",
    "Conditional selection works in the same way it does with numpy arrays.\n",
    "Example:\n",
    "    df = pd.DataFrame(np.array([[-1, 2, 3, -4], [-5, 6, -7, 8], [9, 10, 11, 12], [13, -14, -15, -16], [-17, -18, -19, -20]]), \\\n",
    "                  ['A', 'B', 'C', 'D', 'E'], ['W', 'X', 'Y', 'Z'])\n",
    "    >>>    W   X   Y   Z\n",
    "        ________________\n",
    "        A -1   2   3  -4\n",
    "        B -5   6  -7   8\n",
    "        C  9  10  11  12\n",
    "        D 13 -14 -15 -16\n",
    "        E-17 -18 -19 -20         \n",
    "            \n",
    "Will return the whole dataframe with the number when the condition is true and \"NaN\" when the condition is false:\n",
    "    df[df>0]\n",
    "    >>>    W   X   Y   Z\n",
    "        ________________\n",
    "        A NaN  2   3 NaN\n",
    "        B NaN  6  NaN 8\n",
    "        C  9  10  11  12\n",
    "        D 13 NaN NaN NaN\n",
    "        E NaN NaN NaN NaN\n",
    "            \n",
    "This will not happen when we pass the condition as a row condition or a column condition. When we pass conditions based off of columns we'll get series values, meaning we'll get only the rows were happens to be true.\n",
    "    df['W']>0\n",
    "    >>> A False\n",
    "        B False\n",
    "        C True\n",
    "        D True\n",
    "        E False\n",
    "        \n",
    "    df[df['W']>0]\n",
    "    >>>    W   X   Y   Z\n",
    "        ________________\n",
    "        C  9  10  11  12\n",
    "        D 13 -14 -15 -16\n",
    "######################################\n",
    "When we want to pass more than one condition we should write each condition in parenthesis.\n",
    "Python conditional operators \"and\" or \"or\", only work with single statements and not series. When using them in series we should use \"&\" for \"and\" and \"|\" for \"or\".\n",
    "Example:\n",
    "    df[(df['W']>0) & (df['Y')>1)]\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Index and Multiple indexing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If we have assigned some customized indexing and we want to reset it back to default (which means just a numerical index) we use:\n",
    "    df.reset_index(inplace = True)\n",
    "######################################\n",
    "If on the contrary we want to change the index to another existint column:\n",
    "    df.set_index('col_name', inplace = True)\n",
    "######################################\n",
    "The function zip(list1, list2) will pick every item in list1 and list2 and put it by pair in a tuple list. This will be useful when creating a multiple index.\n",
    "\n",
    "To create a multiple index data frame we use the command:\n",
    "    pd.MultiIndex.from_tuples(zipped_tuple_list)\n",
    "Then, passing that as an index when creating the data frame, we will have a multiindex data frame.\n",
    "Example:\n",
    "    list1 = ['G1', 'G1', 'G1', 'G2', 'G2', 'G2']\n",
    "    list2 = [1, 2, 3, 1, 2, 3]\n",
    "    multi_index = pd.MultiIndex.from_tuples(zip(list1, list2))\n",
    "    data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "    \n",
    "    df = pd.DataFrame(data, multi_index, ['A', 'B'])\n",
    "    >>>\n",
    "           A   B\n",
    "    ______________\n",
    "    G1 1 | 1   2\n",
    "       2 | 3   4\n",
    "       3 | 5   6\n",
    "    G2 1 | 7   8\n",
    "       2 | 9   10\n",
    "       3 | 11  12\n",
    "######################################\n",
    "To access data from a multi index we start calling data from the outer index to the inner ones.\n",
    "Example:\n",
    "    df.loc['G1'].loc[1]\n",
    "    >>> [1, 2]\n",
    "######################################\n",
    "To give name to the indexes we use:\n",
    "    df.index.names = ['Groups', 'Num']\n",
    "    >>> \n",
    "                A   B\n",
    "    __________________\n",
    "    Groups Num\n",
    "     G1     1 | 1   2\n",
    "            2 | 3   4\n",
    "            3 | 5   6\n",
    "     G2     1 | 7   8\n",
    "            2 | 9   10\n",
    "            3 | 11  12\n",
    "######################################\n",
    "To grab data from a multi index data frame we cal also use the cross-section function.\n",
    "Example:\n",
    "    df.xs(1, axis = 0, level = 'Num')\n",
    "    >>> [[1, 2], [7, 8]]\n",
    "    \n",
    "It grabbed all the data with the indexes 1 from the second index, the one at the level 'Num'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Missing data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"np.nan\" allows to add NaN if we want to simulate some missing value for some explanatory purpose.\n",
    "######################################\n",
    "Given a dataframe we can eliminate all rows or columns containing missing values with:\n",
    "    df.dropna(axis = {0, 1})\n",
    "    \n",
    "0 for the rows and 1 for the columns.\n",
    "######################################\n",
    "For specifying a threshold we can use the argument \"thresh\".\n",
    "    df.dropna(axis = {0, 1}, thresh = x)\n",
    "    \n",
    "This will drop all columns or rows where the number NaN >= x.\n",
    "######################################\n",
    "If we want to fill the missing data with a value, we use:\n",
    "    df.fillna(value = x)\n",
    "######################################\n",
    "We can replace the NaN of a row or column with, for example, the mean of that row or column.\n",
    "\n",
    "    df['A'].fillna(value = df['A'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Group By"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This, allows to group multiple rows or columns into one singular value.\n",
    "\n",
    "It gathers several values grouped by column or row and aggregates them using some aggregate function like the sum, mean or standard deviation.\n",
    "\n",
    "To create a \"group by\" object from a dataframe \"df\" we do:\n",
    "    df.groupby('col_name')\n",
    "######################################\n",
    "To such objects we can apply some aggregate functions to perform operations. Notice that operations will be performed for numerical columns only, string columns will automatically not be taken into account by pandas.\n",
    "    df.groupby('col_name').mean()\n",
    "    df.groupby('col_name').sum()\n",
    "    df.groupby('col_name').std()\n",
    "    df.groupby('col_name').count()\n",
    "    df.groupby('col_name').min()\n",
    "    df.groupby('col_name').max()\n",
    "######################################\n",
    "    df.groupby('col_name').describe()\n",
    "    \n",
    "This will provide a bunch of data such as mean, std, maximum, minimum, quartile values. This can be trasnposed for better visualization with the \".transpose()\" method.\n",
    "######################################\n",
    "We can group by different columns:\n",
    "    df.groupby(by = ['col_name1', 'col_name2']).aggregate_function()\n",
    "    \n",
    "When doing so we will create a multi index. To go back to a single index we can use the \"unstack()\" method. It will return a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Merging, Joining and Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Concatenation glues together the DataFrames. Keep in mind that dimensions should match along the axis where we are concatenating. For that we use the command:\n",
    "    pd.concat([df1, df2, ...], axis = {0, 1}]\n",
    "    \n",
    "0 for rows and 1 for columns.\n",
    "######################################\n",
    "When we merge, sometimes we want to do it around a key column so instead of concatenating we join both dataframes in a column they share. This is done through the command:\n",
    "    pd.merge(df1, df2, how = 'inner', on = 'key_col')\n",
    "    \n",
    "We can also merge around several keys by passing a list of keys.\n",
    "######################################\n",
    "Joining is a convenient method for combining the columns of two potentially differently indexed dataframes into a single resulting dataframe. It is the same as merging except that now the keys are on the index instead that on the column.\n",
    "    df1.join(df2)\n",
    "    \n",
    "This will join df2 to df1 in the index of df1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Pandas Common Operations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To obtain the unique values we can use the following methods:\n",
    "    df['col_name'].unique() --> To obtain an array containing all the unique values.\n",
    "    df['col_name'].nunique() --> To obtain a count of how many unique values there are in your column.\n",
    "    df['col_name'].value_counts() --> Returns how many times each unique value occurred in that column.\n",
    "######################################\n",
    "To obtain the index where maximum and minimum values occur:\n",
    "    df.idxmin()\n",
    "    df.idxmax()\n",
    "###################################### \n",
    "If we have built a custom function and we want to apply it to a data frame we can do that by using:\n",
    "    df['col_name'].apply(function_name)\n",
    "\n",
    "Example:\n",
    "        def times2(x):\n",
    "            return x*2\n",
    "            \n",
    "        df['col1'].apply(times2)\n",
    "######################################\n",
    "Analogous to the previous concept, we can aso apply built-in python functions to the data frames, for example we can do:\n",
    "    df['col_name'].apply(len)\n",
    "    \n",
    "This will return the length of the column.\n",
    "This can also work with lambda expressions.\n",
    "######################################\n",
    "To return information about the columns and indexes we use:\n",
    "    df.columns\n",
    "    df.index\n",
    "######################################\n",
    "To sort the values of a dataframe according to a certain row or columns we can do:\n",
    "    df.sort_values(by = 'col1', axis = {0, 1}, ascending = {True, False})\n",
    "    \n",
    "This will sort all the values of the dataframe by column 1, remember that 0 is for rows and 1 for columns.\n",
    "######################################\n",
    "To locate null values we can use the method \".isnull\" and it will return a boolean data frame indicating True or False if there is or there is not NaN.\n",
    "######################################\n",
    "Given a data frame we can use the pivot table mehtod in order to create new index and multindex columns.\n",
    "\n",
    "Example:\n",
    "    df = pd.DataFrame(np.array([[-1, 2, 3, -4], [-5, 6, -7, 8], [9, 10, 11, 12], [13, -14, -15, -16], [-17, -18, -19, -20]]), \\\n",
    "                  ['A', 'B', 'C', 'D', 'E'], ['W', 'X', 'Y', 'Z'])\n",
    "                  \n",
    "   >>>    W   X   Y   Z\n",
    "        ________________\n",
    "        A -1   2   3  -4\n",
    "        B -5   6  -7   8\n",
    "        C  9  10  11  12\n",
    "        D 13 -14 -15 -16\n",
    "        E-17 -18 -19 -20\n",
    "        \n",
    "   df.pivot_table(values = 'Z', index = ['W', 'X'], columns = ['Y'])\n",
    "   >>>      Y   -19   -15   -7    3   11\n",
    "       W    X   \n",
    "      -17  -18  -20   NaN   NaN  NaN  NaN\n",
    "      -5    6   NaN   NaN    8   NaN  NaN\n",
    "      -1    2   NaN   NaN   NaN   4   NaN\n",
    "       9    10  NaN   NaN   NaN  NaN   12\n",
    "      13   -14  NaN   -16   NaN  NaN  NaN\n",
    "\n",
    "This will use 'W' and 'X' as a multiindex, 'Y' as the column and the present values in the dataframe will be the values corresponding of column 'Z'.\n",
    "######################################\n",
    "To transform a given column in string format to date format:\n",
    "    pd.to_datetime()\n",
    "######################################\n",
    "To map a column:\n",
    "    df['col_name'] = df['col_name'].map(maping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Data Input and Output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To check the location of the current Jupyter Notebook file we use:\n",
    "    pwd\n",
    "######################################\n",
    "We can read and save different format files. If the file is in the same directory than the jupyter notebook, we just have to type the file name and extension, on the contrary if it is in another directory we will have to specify the whole path.\n",
    "######################################\n",
    "csv files:\n",
    "    pd.read_csv('filename.csv')\n",
    "    pd.to_csv('filename', index = {True, False})\n",
    "    \n",
    "When saving as csv it is better to use index = False, that way the index is not stored as a column of the dataframe. With index = True, the index will be stored as another column and when reopening the file, we will have an extracolumn that we will have to convert to the index. So if the index is just a numerical standard index it is better not save it, and when reopening the file again python will assign again the numerical index by default. On the contrary if the index is a formated one or something we do not want to lose we can save it and when opening the file again, use pandas to assign a new index to that column.\n",
    "\n",
    "When we read from a csv file, we can specify which column do we want to be the index. That way we avoid pandas adding a numerical index of its own.\n",
    "    pd.read_csv('filename', index_col = 0)\n",
    "\n",
    "Here we are setting column 0 as the index. This is useful for a time series as the first column is usually the date and we can set it as the time index.\n",
    "######################################\n",
    "Excel files:\n",
    "To read from an Excel file the philosophy is the same as before but we need to take into account that pandas is only able to read data from Excel, not formulas or macros.\n",
    "\n",
    "    pd.read_excel('filename.xlsx', sheet_name = 'Name_of_Sheet')\n",
    "    pd.to_excel('filename.xlsx', sheet_name = 'Name_of_Sheet')\n",
    "    \n",
    "When reading an Excel file it is better to specify the sheet name where the data is contained in order for the pandas reader not to crash. When writing the data, if we do not specify the name, a name by default will be given.\n",
    "######################################\n",
    "html files:\n",
    "For this, we probably need to install some extra libraries.\n",
    "    pd.read_html('html_link')\n",
    "    \n",
    "This will not produce a data frame, this will produce a list. We need to iterate that list to see where is the data we are looking to. This will usually be in the position 0 --> data[0].\n",
    "######################################\n",
    "sql files:\n",
    "If working with sql databases, pnadas may not be the best way to import and read the data. We should use specific libraries for each flavour of sql. Nevertheless here we will show how can we create a sql engine in memory to read data from sql if we have to.\n",
    "\n",
    "Example:\n",
    "    from sqlalchemy import create_engine\n",
    "    engine = create_engine('sqlite:///:memory:')\n",
    "    \n",
    "    pd.read_sql('Name', con = engine)\n",
    "    df.to_sql('Name', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Data sources and pandas datareader "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pandas datareader is a separate package that allow to connect to some APIs, grab its data and read it as a dataframe. Here we will see examples with securities data from Google and Yahoo APIs. Yahoo and Google have changed their APIs and sometimes are unstable, not able to acces or even deprecated, instead \"iex\" and \"morningstar\" are other sources.\n",
    "\n",
    "Also \"Quandl\" will be used which is a company that offers robust python APIs that allows to grab data from a variety of sources (either free or paid). Quandl is free to use, however if you want to actually access free data more than 50 times a day you will need to create and API key by registering in Quandl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on APIs and string codes in pandas datareader we can access: https://pandas-datareader.readthedocs.io/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To import the library:\n",
    "    import pandas_datareader.data as web\n",
    "    import datetime ---> To pass datetime objects.\n",
    "######################################\n",
    "To use datetime objects we can define any date as:\n",
    "    datetime.datetime(year, month, day)\n",
    "######################################\n",
    "To use the data reader for obtaining a dataframe we do:\n",
    "    df = Web.DataReader('ticker', data_source = 'source', start = 'yyyy-mm-dd', end = 'yyyy-mm-dd')\n",
    "\n",
    "Example:\n",
    "    facebook = Web.DataReader('FB', data_source = 'yahoo', start = '2005-01-01', '2020-01-01')\n",
    "    \n",
    "As an start and end, we can pass a string like we saw or also a datetime object.\n",
    "######################################\n",
    "NOTE: As of v07.0 of pandas datareader, Google Finance and Morningstar have been immediately deprecated due to large changes in their APIs and no stable replacements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 Quandl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.quandl.com "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Quandl is a company that charges for its premium services, nevertheless there are things that can be done for free. To acces the API more than 50 times a day we need to register, obtain an API key and include it in the requests.\n",
    "\n",
    "Example:\n",
    "    data = quandl.get('WIKI/Ticker', api_key = ' ')\n",
    "    \n",
    "If we do not want all the columns but just a single one, we can select it directly when grabbing the data to avoid further postprocessing:\n",
    "    data = quandl.get('WIKI/Ticker.col', api_key = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.11 Time Series Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The majority of our data is going to be in the form of a time series as many times we will analyze the temporal evolution of some variable. That means that we will have a date time index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.12 DateTime"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datetime is the Python built-on date and time library that will allow to create timestamps or day objects.\n",
    "    from datetime import datetime\n",
    "    datetime(year, month, day, hour, minute, second)\n",
    "\n",
    "NOTE: Careful from the \".datetime\" method and library, as it is not the same to use:\n",
    "        import datetime\n",
    "             than\n",
    "        from datetime import datetime.\n",
    "      \n",
    "      For the former we will have to call the method from within the library as datetime.datetime( ).\n",
    "      For the latter we can directly call the \"datetime\" command as is the only one imported.\n",
    "\n",
    "The previous example creates a datetime.datetime object. From this object we can call several attributes like:\n",
    "    .day\n",
    "    .month\n",
    "    .year\n",
    "    .hour\n",
    "    .minute\n",
    "    .second\n",
    "######################################\n",
    "From a list we can create a datetime index with:\n",
    "    pd.DatetimeIndex(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.13 Time resampling "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When importing a data frame sometimes the date is not a datetime object, but a string. We can convert this into a datetime object:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "If the string date format is not: year, month, day the actual date format should be specified through the argument:\n",
    "    strftime = '%d/%m/%Y' or the given format.\n",
    "    \n",
    "Once done that remember that the date column can be set as the index with the command:\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "Another way of doing this is directly when reading the dataframe:\n",
    "    df = pd.read_csv('filename.csv', index_col = 'Date', parse_date = True)\n",
    "\n",
    "This will only work if the data is given in an expected format. But we can always try it before implementing the previous because if that works it will save a lot of time and a couple lines of code.\n",
    "\n",
    "We will usually get the data on a smaller time scale than we desire like day or hour, however it is often a good idea to aggregate the data based off some frequency like weekly, monthly or quarterly.\n",
    "\n",
    "To do a time resampling we need a date time index, ALWAYS. To do the resampling we use the command:\n",
    "    df.resample(rule = 'A').mean()\n",
    "\n",
    "The 'rule' argument determines by what interval the data will be resampled. It is gotten from a table of string aliases that refer to different resampling intervals like:\n",
    "    'A' or 'Y' for year.\n",
    "    'D' for day.\n",
    "    'Q' for quarter.\n",
    "    'BD' for bussiness days.\n",
    "And many other that can be consulted in the link posted below.\n",
    "\n",
    "The method following the resampling method indicates the function to group the values in the resampling time frame. In this case we opted for the mean, but the standard deviation, min, max values, etc... can also be returned. If on the contrary we want to return date based on a custom function created by ourserlves we can do it by:\n",
    "    df.resample(rule = 'A').apply(custom_function)\n",
    "    \n",
    "Resampling is a very useful concept, because we can later on plot based on that resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String aliases table for time resampling: https://towardsdatascience.com/using-the-pandas-resample-function-a231144194c4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.14 Time shifting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Certain models or calculations will require us to shift our data forward or backward a certain amount of time steps. When we want to shift our data some index position we call in for the method:\n",
    "    df.shift(periods = x)\n",
    "    \n",
    "This will shift our data x periods, basically x row positions. Positive numbers will shift the data forward while negative numbers will shift the data backwards.\n",
    "\n",
    "Example:\n",
    "    df\n",
    "    >>>     W   X\n",
    "        ___________\n",
    "        A  -1   2\n",
    "        B  -5   6\n",
    "     \n",
    "     df.shift(1)\n",
    "     >>>    W   X\n",
    "        ___________\n",
    "        A  Nan  Nan\n",
    "        B  -1    2\n",
    "        \n",
    "     df.shift(-1)\n",
    "     >>>    W    X\n",
    "        ____________\n",
    "        A  -1    2\n",
    "        B  Nan  Nan\n",
    "\n",
    "Be aware that by doing this we will lose the first or last data equivalent to the shifting periods.\n",
    "\n",
    "Example:\n",
    "This can be used to calculate daily returns given by the formula: \n",
    "    R_t = (P_t - P_{t-1}) / P_{t-1} = (P_t / P_{t-1}) - 1\n",
    "    \n",
    "Which can be calculated as:\n",
    "    df['Returns'] = df['Price'] / df['Price'].shift(1) - 1\n",
    "    \n",
    "We can also shift all the particular rows to matching some time string code provided by means of a frequencyargument. It is as shifting everything on the index to match the same stream code.\n",
    "    df.tshift(freq = 'M')\n",
    "\n",
    "The frequency can be montly ('M'), annually ('A'), quarterly ('Q') or many other...\n",
    "\n",
    "In this case, we do not lose any data because what we are actually shifting is the index. We change the index so all the data has the desired common index. It acts as a groupby but where we reassign the index.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.15 Rolling and Expanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.15.1 Rolling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sometimes we may need to use rolling methods to create things like a rolling mean (or moving average) to get more signal about the general trend of the data. We provide a window of a set time period and use the date falling inside such window to calculate the desired aggregate statistic such as the mean.\n",
    "\n",
    "To use a rolling method we do:\n",
    "    df.rolling(window = x).function\n",
    "\n",
    "Where 'x' is the size of the moving window, that is, the number of observations used for calculating the statistic, and 'function' corresponds to the desired function calculation.\n",
    "\n",
    "Notice that for a window of x points, the first x-1 points of information will be lost. This is because at each point the statistics uses current point and previous x-1 points, so, for the first x-1 points we do not have enough data to produce a result.\n",
    "\n",
    "The best way to calculate and plot that is by creating a new column in the dataframe containing the rolling data:\n",
    "    df['rolling_mean'] = df['price'].rolling(window = 30).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.15.2 Expanding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If we want to take into account everything since the beginning of the time where everypoint contains the information of all previous points, we have to use the \".expanding\" method instead of the \".rolling\".\n",
    "    df['expanding'] = df['price'].expanding().mean()\n",
    "\n",
    "Expanding is not used that much because there may be some high peaks and huge drops that end cancelling out and not reflecting the whole behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.15.3 Example: Bolinger Bands"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The bolinger bands were developed by John Bolinger. They are volatility bands placed above and below a moving average. The volatility of a financial assets is based on the standard deviation of its development. The bands will widen when volatility increases and will narrow when volatility decreases (more stable asset). Bollinger bands can be used to identify tops and bottoms and the strength of the actual trend.\n",
    "\n",
    "As bollinger bands are based on the volatility they reflect better the variation than the mean. Generally relatively high prices are considered when they are above the upper Bollinger band and relatively low prices when they are below the lower Bollinger band, nevertheless Bollinger bands are not a standalone tool and further indicators should be considered.\n",
    "\n",
    "Contrary to popular belief, Bollinger bands cannot be used to make reliable predictions based on how close or far will the price of the asset be with respect to their mean. This is because the price of an asset is not driven by any statistical distribution function known. Calculating the bands with 2 times the standard deviation does not ensure that 95% of closing prices will be inside the bands, that would require that prices follow a Gaussian distribution, which does not happen in reality.\n",
    "\n",
    "Nevertheless Bollinger bands are very useful analysing the price of an asset and because of the Chebyshev inequality they contain at least 75% of the prices.\n",
    "\n",
    "We should not give a special meaning when a price reaches upper or lower band, this cases should be analyzed together with other factors and metrics, nevertheless the interpretation that of a price reaching a band was so popular that some investment funds use them to take their positions and by doing so they are driving the market and giving meaning to this kind of events that in any other way would not occur.\n",
    "\n",
    "Typically to calculate a Bollinger band we use the 20 day moving average and 2 times the standard deviation in that period:\n",
    "    Upper_band = 20_day_moving_average + 2*20_day_std\n",
    "    Lower_band = 20_day_moving_average + 2*20_day_std\n",
    "    \n",
    "Python:\n",
    "    df['close_avg'] = df['close'].rolling(20).mean()\n",
    "    df['upper_bollinger'] = df['close'].rolling(20).mean() + 2*df['close'].rolling(20).std()\n",
    "    df['lower_bollinger'] = df['close'].rolling(20).mean() - 2*df['close'].rolling(20).std()\n",
    "    df[['close', 'close_avg', 'upper_bollinger', 'lower_bollinger']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MATPLOTLIB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation User Guide: https://matplotlib.org/stable/users/index.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Matplotlib is a 2D plotting library which produces quality figures in a variety of hard copy formats, that means we can export any figure to PNG, JPEG, or many other formats.\n",
    "\n",
    "You can generate plots, histograms, power spectra plots, bar, charts, error charts, scatterplots, etc... It is the most powerful and customizable visualization library for Pyhton.\n",
    "\n",
    "Matplotlib was initially based off Matlab's plotting capabilities.\n",
    "\n",
    "It has two API structures, one is the objected oriented API structure and the other one is the function oriented structure.\n",
    "\n",
    "Matplotlib is imported as:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline ---> To see the plots inside Jupyter Notebook.\n",
    "    %matplotlib notebook ---> To see the plots in an interactive way inside a window with a cursor with options, zoom-in, zoom-     out, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Functional Approach"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(x, y)\n",
    "plt.xlabel('insert_label')\n",
    "plt.title('insert_title')\n",
    "plt.show()\n",
    "plt.xlim[Xmin, Xmax]\n",
    "plt.ylim[Ymin, Ymax]\n",
    "\n",
    "plt.subplot(nrows, ncolumns, plot_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Object Oriented Approach"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    fig = plt.figure()\n",
    "    axes = fig.add_axes([l, b, w, h]) left, bottom, width and height. Percentage ([0,1]) we want to dedicate to the plot.\n",
    "\n",
    "Basically we have created a figure object and added some axis. Now to create a plot with some values, we want to plot over the set of axis we have previously created:\n",
    "\n",
    "    axes.plot(x, y)\n",
    "    axes.set_xlabel('insert_label')\n",
    "    axes.set_title('insert_title')\n",
    "######################################   \n",
    "To combine several plots we just have to create different axes inside the same figure and plot on them:\n",
    "    axes1 = fig.add_axes([l, b, w, h])\n",
    "    axes2 = fig.add_axes([l, b, w, h])\n",
    "    \n",
    "    axes1.plot(x1, y1)\n",
    "    axes2.plot(x2, y2)\n",
    "######################################   \n",
    "To create subplots using and object oriented method we use:\n",
    "    fig, axes = plt.subplots(n_rows, n_cols)\n",
    "    plt.tight_layout() ---> This is included at the end of all the plot statements and it's used to avoid overlaping of axis.\n",
    "    \n",
    "When doing that, the axes object is just an array of size n_rows x n_cols containing several axis objects corresponding each one to the different subplots.\n",
    "\n",
    "To plot in the different subplots we just use the \".plot\" method in each of the axes objects.\n",
    "    axes[0].plot(x, y)\n",
    "    axes[1].plot(x, y)\n",
    "    ...\n",
    "######################################    \n",
    "As well as the \".plot\" method we can use all the other methods seen previously in each of the axis objects to add labels and titles.\n",
    "######################################\n",
    "We can adjust the figure size and the pixels per inch with the arguments \"figsize = (w, h)\" and \"dpi = n\". This arguments go inside the figure or the subplot method:\n",
    "    fig = plt.figure(figsize = (w, h), dpi = n)\n",
    "    fig, axes = plt.subplots(figsize = (w, h), dpi = n)\n",
    "######################################    \n",
    "To add plot legends:\n",
    "Labels for each line are added as an argument in the plot method or function and then called via the \".legend\" or \"plt.legend\" methods.\n",
    "\n",
    "    axes.plot(x, y, label = 'title')\n",
    "    axes.legend(loc = 0) ---> The argument 0 will chose the best location. If not, we can input the location by specifying a       tuple loc = (x, y), where x and y are the coordinates from the left bottom corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Save a figure into a file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To save a figure into a file we do:\n",
    "    fig.savefig('file_name.extension', dpi = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Customization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can customize the color, line, marker, etc... To do it, we do it via arguments inside the \".plot\" method.\n",
    "######################################\n",
    "    To change the color: color = '#FF8C00' or c = '#FF8C00' ---> By inputing the RGB color code. Also you can input some           predefined colors in the library like 'red', 'blue', 'black', ...\n",
    "    To change the linewidth: linewidth = n or lw = n\n",
    "    To change the transparency: alpha = 0.5\n",
    "    To change linestyle: linestyle = '--' or ls = '--' ---> '--' will produce a dashed line and '-.' a dash dotted line and '-'     will produce a solid line. There are many other linestyles that can be searched.\n",
    "######################################    \n",
    "Remember that the plot is just an array of points joined together by a line. If we want to show where the points are we can display a marker and changes its properties via the following arguments:\n",
    "    \n",
    "    To set the marker style: marker = 'O' ---> There are many marker styles like 'O', '*', '^' and many others that can be         checked.\n",
    "    To set the marker size: markersize = n\n",
    "    To set the marker face color: markerfacecolor = '#FF8C00' ---> By inputing the RGB color code. Also you can input some         predefined colors in the library like 'red', 'blue', 'black', ...\n",
    "    To set the marker edge width: markeredgewidth = n\n",
    "    To set the marker edge color: markeredgecolor = '#FF8C00' ---> By inputing the RGB color code. Also you can input some         predefined colors in the library like 'red', 'blue', 'black', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Axis range "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To set the axis limit of any axis we use:\n",
    "    axes.set_xlim([Xmin, Xmax])\n",
    "    axes.set_ylim([Ymin, Ymax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Other plots"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Other kind of possible visualizations we can use, are:\n",
    "    \".scatter( )\" ---> To plot scatter plots.\n",
    "    \".hist( )\" ---> Histogram plots.\n",
    "    \".boxplot( )\" ---> Box quartile plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. PANDAS + MATPLOTLIB VISUALIZATION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can plot the histogram of a column data frame with the method \".hist\":\n",
    "    df['A'].hist(bins = 30)\n",
    "    \n",
    "This will plot the histogram, of column A of dataframe \"df\" with 30 histogram bars.\n",
    "######################################\n",
    "A way to produce different plot types out of a dataframe is:\n",
    "    df['A'].plot(kind = 'hist')\n",
    "    df['A'].plot(kind = 'area', stacked = {True, False})\n",
    "    df['A'].plot(kind = 'bar')\n",
    "    df['A'].plot.hist()\n",
    "    df['A'].plot.area()\n",
    "    df['A'].plot.bar()\n",
    "    \n",
    "The area plot is like a line plot but where the area below has been colored.\n",
    "In the plot to bar we can use: df['A'].plot.bar(stacked = {True, False}). With stacked = False, the bars will be placed one next to the other and with stack = True, they will be placed one on top of the other.\n",
    "######################################\n",
    "To create a line (2-variable plot) we have to specify the \"x\" and the \"y\" axes:\n",
    "    df1.plot.line(x = df1.index, y = 'B')\n",
    "\n",
    "In this case, out of the dataframe \"df1\" we will be producing a line where \"x\" axis will be the index and the \"y\" axis will be the column 'B'. Inside this, as seen before, we can call all the available matplotlib arguments like the color, linewidth, linestyle, etc...\n",
    "######################################\n",
    "The same is applicable to scatter plots:\n",
    "    df1.plot.scatter(x = 'A', y = 'B')\n",
    "    \n",
    "Here, the color can be displayed as depending of another column, for example, column 'C'.\n",
    "    df1.plot.scatter(x = 'A', y = 'B', c = 'C')\n",
    "    \n",
    "By default the color scale is black and grey but we can change this with the color map argument, \"cmap\", for example:\n",
    "    df1.plot.scatter(x = 'A', y = 'B', c = 'C', cmap = 'coolwarm') ---> 'coolwarm' is an example of a predefined colormap but       we can find many other color configurations.\n",
    "    \n",
    "Instead of by color, we can also display it by size of a given column:\n",
    "    df1.plot.scatter(x = 'A', y = 'B', s = df1['C']*100) ---> We use a scaling factor of 100 because otherwise the points would     be too small.\n",
    "######################################\n",
    "To obtain a box plot:\n",
    "    df1.plot.box()\n",
    "    \n",
    "By passing this method, by default a box plot will be created for each column.\n",
    "######################################\n",
    "Another kind of plot is the hexagonal plot:\n",
    "    df1.plot.hexbin(x = 'A', y = 'B', gridsize = 25)\n",
    "    \n",
    "This produces like a scatter plot but with hexagonal shape and a colormap.\n",
    "######################################\n",
    "A kernel density plot is similar to an histogram, if we want a kernel density distribution we use:\n",
    "    df1['A'].plot.kde()\n",
    "\n",
    "This, instead for a given column, can also be done for the whole dataframe, not just one column. If we pass the entire dataframe the plot will produce as many lines as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Pandas Time Series Visualization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If we just do df['col1'].plot() this will do a simple line plot of values of 'col1' of the dataframe versus the index of the dataframe.\n",
    "\n",
    "The plot command is the same than before just that this time it is being called from a pandas method and therefore the arguments are equivalent.\n",
    "    df['col1'].plot(xlim = [xmax, xmin], ylim = [ymax, ymin], figsize = n, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.1 Date changing:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If we want to change the way dates are shown in a plot, we can do it using a new library:\n",
    "    import matplotlib.dates as dates\n",
    "    \n",
    "To plot and adjust automatically the best suitable date formate we use the method \".autofmt_xdate()\".\n",
    "Example:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes()\n",
    "    ax.plot(index, values, '-')\n",
    "    fig.autofmt_xdate()\n",
    "    ax.xaxis.grid(True)\n",
    "    ax.yaxis.grid(True)\n",
    "######################################\n",
    "Now if what we want is to change the date format in which information is displayed, the first thing we do is we use the locator to locate the time format in the time format we want. The second step will be to format the data which we will actually put in the plot axis.\n",
    "Example:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes()\n",
    "    ax.plot_date(index, value, '-')\n",
    "    ax.xaxis.set_major_locator(dates.MonthLocator()) ---> Capital letter produces the whole word while lower will abbreviate.\n",
    "    ax.xaxis.set_major_formatter(dates.DateFormatter('\\n%b-%y')) ---> 'b' for month and 'y' for year.\n",
    "    ax.xaxis.set_minor_locator(dates.WeekdatLocator(byweekday = 0)) ---> 0 will locate first day as monday.\n",
    "    ax.xaxis.set_minor_formatter(dates.DateFormatter('%d'))\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "This will produce a set of axis in this way:\n",
    "\n",
    "  |----|----|----|----|----|----|----|----|----|----|----|----|\n",
    " 01   08   15   22   29   05   12   19   26   05   12   19   26\n",
    "  Jan 2007                 Feb 2007            Mar 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. SEABORN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation User Guide: https://seaborn.pydata.org/index.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Seaborn is a statistical plotting library built on top of matplotlib. It is designed to work very well with pandas\n",
    "dataframe.\n",
    "\n",
    "To install it type in the prompt:\n",
    "   conda install seaborn\n",
    "   or\n",
    "   pip install seaborn\n",
    "\n",
    "The usual way of importing seaborn is as \"sns\":\n",
    "   import seaborn as sns\n",
    "\n",
    "The seaborn library has some defined datasets that can be loaded and used to learn, some examples are:\n",
    "   flights = sns.load_dataset('flights') ---> contains data about the passengers of some flights, its year and month\n",
    "   tips = sns.load_dataset('tips') ---> contains data about the tips left at a restaurant.\n",
    "   iris = sns.lead_dataset('iris') ---> contains data about three species of flours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Distribution Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1.1 Dist Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The distplot shows the distribution of a univariate (one variable) set of observations:\n",
    "   sns.distplot(dataframe['col'], kde = {True, False}, bins=x)\n",
    "\n",
    "In an histogram if the number of bins is to high, we will lose the information about the distribution as we\n",
    "will obtain a plot of all the values, the key is to find a balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1.2 Joint Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "jointplot allows you to match two plots for bivariate (two variables) data. Here you can combine two different\n",
    "distribution plots into another plot of your choice.\n",
    "   sns.jointplot(x = 'col1', y = 'col2', data = dataframe, kind = {'scatter', 'reg', 'resid', 'kde', 'hex'} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1.3 Pair Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pairplot is going to plot pairwise relationships across an entire dataframe for the numerical columns, and supports a color hue argument for categorical columns.\n",
    "pairplot is going to do what jointplot does but for every possible combination of all the columns in your dataframe.\n",
    "   sns.pairplot(dataframe)\n",
    "\n",
    "When pairplot plots a variable against the same variable it shows the histogram instead of the scatter graph \n",
    "(which will basically be a straight line).\n",
    "\n",
    "If there are no arguments, pairplot plots only the numerical columns, if we want the information based on some categorical \n",
    "column, we have to specify the category as an input and it will be colored acordingly.\n",
    "   sns.pairplot(dataframe, hue = 'cat_col', palette = color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1.4 Rug Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rugplots just draw a dash mark for every existing point in a univariate distribution. They are the way of building a kernel \n",
    "density estimation plots.\n",
    "   sns.rugplot(dataframe['col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1.5 kde Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It just plots the kernel density estimation line.\n",
    "   sns.kdeplot(dataframe['col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Categorical plots"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Categorical plots are used to plot a set of categorical data in reference to either one numerical column or another\n",
    "categorical column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.1 Bar Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "barplot is a general plot that allows you to aggregate the categorical data based off some function, \n",
    "by default the mean:\n",
    "   sns.barplot(x = 'cat_col', y = 'num_col', data = dataframe, estimator = {np.mean, np.std, ...})\n",
    "\n",
    "This will allow you to visualize the mean, standard deviation, etc... of the numerical set (num_col_) \n",
    "by the categorical set (cat_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.2 Count Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "countplot is like a barplot where the estimator is by default a counting function of the ocurrencies\n",
    "   sns.countplot(x = 'cat_col', data = dataframe)\n",
    "\n",
    "This is analogous as plotting the result of pandas.count(dataframe['cat_col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.3 Box Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "boxplots are used to shown the distribution of categorical data. A box plot (or box-and-whisker plot) shows the distribution \n",
    "of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable. \n",
    "The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, \n",
    "except for points that are determined to be outliers using a method that is a function of the inter-quartile range.\n",
    "\n",
    "    sns.boxplot(x = 'cat_col1', y = 'num_col', data = dataframe, hue = 'cat_col2')\n",
    "If desired we can add via the argument \"hue\" another set of categorical data, which will add a new layer of information\n",
    "to our plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.4 Violin Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several \n",
    "levels of one (or more) categorical variables such that those distributions can be compared. Unlike a box plot, \n",
    "in which all of the plot components correspond to actual datapoints, the violin plot features a kernel density \n",
    "estimation of the underlying distribution.\n",
    "\n",
    "As the information in the violin plot is doubled (symmetrical) if we add a new layer of information, by plotting two\n",
    "categorical data sets, we can split this information at each side of the violin with the argument \"split = True\".\n",
    "\n",
    "   sns.violinplot(x = 'cat_col1', y = 'num_col', data = dataframe, hue = 'cat_col2', split = {True, False})\n",
    "\n",
    "NOTE: Violin plots contain more information and are more difficult to interpret, depending on the audience to which we are\n",
    "      directing the study, sometimes it is better to just go with boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.5 Strip Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The stripplot will draw a scatterplot where one variable is categorical. A strip plot can be drawn on its own, \n",
    "but it is also a good complement to a box or violin plot in cases where you want to show all observations along with \n",
    "some representation of the underlying distribution.\n",
    "\n",
    "One drawback of the stripplot is that you cannot see how many points are stacked over each other, to solve that, we can add\n",
    "an argument \"jitter = True\" which will add some random noise to space a little the points and visualize it better.\n",
    "The \"split\" argument will have the same effect as before, by separating the points belonging to the second category of\n",
    "information.\n",
    "\n",
    "   sns.stripplot(x = 'cat_col1', y = 'num_col', data = dataframe, hue = 'cat_col2', split = {True, False}, \n",
    "                jitter = {True, False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.6 Swarm Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "They are like a combination of violin and strip plots, where the violion plot shows the actual distribution of points.\n",
    "They are not suitable for very large datasets as points start to fall apart and overlap and the computational time\n",
    "to rearray them is too high.\n",
    "\n",
    "   sns.swarmplot(x = 'cat_col1', y = 'num_col', data = dataframe, hue = 'cat_col2', split = {True, False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.7 Combination of plots"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can combine the different plots together by writting them in the same cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2.9 Factor Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All this previous plots can be called by the factor plot by specifying the kind.\n",
    "   sns.factorplot(x = 'cat_col', y = 'num_col', data = dataframe, kind = {bar, violin, box, strip, swarm, ...})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Matrix Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3.1 Heat Map"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order for a heatmap to work properly, the data should already be in a matrix forms, this means that the index name and \n",
    "the column name match up so that the cell value actually indicates something relevant.\n",
    "\n",
    "From a dataframe, we can obtain the data into a matrix form via the method \".pivot_table\":\n",
    "   dataframe.pivot_table(index = 'col1', columns = 'col2', values = 'col3')\n",
    "######################################\n",
    "Then we can construct a heatmap doing:\n",
    "   sns.heatmap(matrix, annot = {True, False}, cmap = {'coolwarm, magma, virids'}, linecolor = 'colour', linewidths = x)\n",
    "\n",
    "The argument \"annot\" is used to add the actual values to the heat in addition to the gradient colouring.\n",
    "The arguments \"linecolor\" and \"linewidths\" are used to specify the color and width of the lines between each cell of the\n",
    "heatmap if desired. If not, no separation line will be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3.2 Cluster Map"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The clustermap uses hierarchal clustering to produce a clustered version of the heatmap, it clusters rows and columns\n",
    "based on their similarity.\n",
    "\n",
    "    sns.clustermap(matrix, standard_scale = 1)\n",
    "\n",
    "The argument \"standard_scale = 1\" is used to normalize the data from 0 to 1. You can also use other scale if desired or\n",
    "do not pass the argument and then it will display the data in their original scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Grids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Grids are general types of plots that allow you to map plot types to rows and columns of a grid, this helps you create \n",
    "similar plots separated by features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4.1 Pair Grid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pairgrid is a subplot grid for plotting pairwise relationships in a dataset. It creates a grid containing all the pair\n",
    "relations of the numerical columns of the dataset.\n",
    "\n",
    "First the grid is created and then different kind of plots can be added to the diagonal, upper or lower part of the grid.\n",
    "   g = sns.PairGrid(dataset)\n",
    "   g.map(plt.scatter)\n",
    "\n",
    "In this case a scatter plot will be represented in all the grids.\n",
    "######################################\n",
    "   g = sns.PairGrid(iris)\n",
    "   g.map_diag(plt.hist)\n",
    "   g.map_upper(plt.scatter)\n",
    "   g.map_lower(sns.kdeplot)\n",
    "\n",
    "In this case different plots will be represented in the diagonal, upper and lower parts. \n",
    "Notice that we just call the plot \"plt.hist\", \"plt.scatter\", \"sns.kdeplot\", this functions have no arguments, as they will\n",
    "automatically plot the data to which the grid corresponds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4.2 Facet Grid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FacetGrid is the general way to create grids of plots based off of a feature.\n",
    "\n",
    "   g = sns.FacetGrid(data = dataset, col = 'catcol1', row = 'catcol2')\n",
    "\n",
    "In this case you need three arguments to create the grid, the data and the categorical sets you want to be distinguished.\n",
    "######################################\n",
    "Once you have done that, you pass the mapping.\n",
    "\n",
    "   g.map(plt.hist, 'numcol1')\n",
    "   g.map(plt.scatter, 'numcol1', 'numcol2')\n",
    "\n",
    "Notice that depending on the type of plot you pass, you will need to specify one or more sets of numerical data, depending if\n",
    "the type of plot is univariable or multivariable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4.3 Joint Grid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "JointGrid is the general version for jointplot() type grids.\n",
    "\n",
    "   g = sns.JointGrid(x = 'numcol1', y = 'numcol2', data = dataset)\n",
    "   g = g.plot(central_plot_type, secondary_plot_type)\n",
    "\n",
    "Where \"central_plot_type\" usually is the regression or scatter plot (\"plt.scatter\" or \"sns.regplot\") and the \n",
    "\"secondary_plot_type\" usually is a distribution or kernel density estimation plot (\"sns.distplot\" or \"sns.kdeplot\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Regression plots"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given a dataset we can do a regression plot using the funcion:\n",
    "   sns.lmplot(x = 'numcol1', y = 'numcol2', data = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.5.1 Hues and grids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can add more information to separate the plot into different categories:\n",
    "   sns.lmplot(x = 'numcol1', y = 'numcol2', data = dataset, hue = 'catcol1')\n",
    "\n",
    "This will plot the different categories in the same plot.\n",
    "######################################\n",
    "If instead we want the different categorical sets to be plot separately into different grids, we can pass the categorical set\n",
    "as a column argument instead of a hue.\n",
    "\n",
    "    sns.lmplot(x = 'numcol1', y = 'numcol2', data = dataset, col = 'catcol1')\n",
    "    \n",
    "The same kind of logic apllies to \"row\" for rows instead of columns or a combination of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.5.2 Markers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"lmplot\" kwargs get passed through to regplot which is a more general form of lmplot(). regplot has a scatter_kws parameter that gets passed to plt.scatter, remember that Seaborn is built with the base of matplotlib. \n",
    "\n",
    "So you want to set the s parameter in that dictionary, which corresponds (a bit confusingly) to the squared markersize. In other words you end up passing a dictionary with the base matplotlib arguments, in this case, s for size of a scatter plot.\n",
    "\n",
    "sns.lmplot(x = 'numcol1', y = 'numcol2', hue = 'catcol', palette = 'coolwarm', markers = ['o', v'], scatter_kws = {'s':100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Aspect, Size, Style and Color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.6.1 Aspect and Size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can change the aspect ratio and size of a plot by adding the following arguments inside the plotting function:\n",
    "\n",
    "   (..., aspect = x, ...)\n",
    "   (..., size = y, ...)\n",
    "   \n",
    "We can also use the matplotlib knowledge to set the figsize as seaborn will call matplotlib for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.6.2 Style"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can change the plot to some defined styles that defined all the plot, these is done by:\n",
    "    sns.set_style('white') ---> For white background\n",
    "    sns.set_style('ticks') ---> Ticks at the edges\n",
    "    sns.set_style('darkgrid') ---> Grey grid background\n",
    "    sns.set_style('whitegrid') ---> White grid background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.6.3 Spines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can remove the lateral or the upper and lower spines with the command:\n",
    "    sns.despine()\n",
    "    \n",
    "By definition upper and right spine are \"True\" to be removed and the other \"False\", but we can change it by input \"left\", \"right\", \"top\" and \"bottom\" as arguments and selecting \"True\" or \"False\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.6.4 Context"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can set different contexts like \"notebook\", \"poster\", \"paper\" and seaborn will adapt our plots, still if we are not satisfied we will be able to change the font size:\n",
    "\n",
    "    sns.set_context({'paper', 'notebook', 'poster', 'talk'}, font_scale = x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.6.5 Palettes and Colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the palette color of a plot with the argument \"palette = 'string'\", to pass the strings that define the different colormaps in the matplotlib documentation: https://matplotlib.org/stable/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. PLOTLY AND CUFFLINKS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Plotly is an interactive visualization library.\n",
    "Cufflinks connects plotly with pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly documentation: https://plotly.com/python/reference/\n",
    "Cufflinks documentation: https://github.com/santosjorge/cufflinks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Necessary imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chart_studio.plotly as py\n",
    "from plotly import __version__\n",
    "\n",
    "As we are going to use plotly offline (there is an online version in order to save an store visualization but it is premium, the offline version is opensource) we need to install:\n",
    " \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "import cufflinks as cf\n",
    "\n",
    "Cufflinks basically connects plotly and pandas via a javascript library, for it to be used in a notebook we need to do:\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "Finally we run a method which will allow to run cufflinks offline:\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cufflinks and pyplot will create beautiful interactive plots of any kind: line plot, scatter plot, bar plot..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1 Scatter plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.iplot(kind='scatter', x='col1', y='col2', mode='markers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2 Bar plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.iplot(kind='bar', x='cat_col', y='values')\n",
    "\n",
    "Bar plot also becomes very powerful when we have non categorical columns. If we have a dataframe with non categorical columns and we call directly:\n",
    "    df.iplot(kind='bar')\n",
    "it is going to plot a bar for every instance of each of the columns. But if we combine it with an aggregate function on the dataframe, we can have more neat visualizations of the data by columns, for example:\n",
    "    df.count().iplot(kind='bar')\n",
    "    df.sum().iplot(kind='bar')\n",
    "\n",
    "This will plot a single bar for each column containing the count or the sum of all the instance of such column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.3 Box plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.iplot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.4 3D surface"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can plot 3 columns of a dataframe in a 3D plot:\n",
    "\n",
    "df[['col1', 'col2', 'col3']].iplot(kind='surface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.5 Histogram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can plot an histogram af a single column of a dataframe:\n",
    "    df['col1'].iplot(kind='hist', bins=25)\n",
    "    \n",
    "We can also pass the entire dataframe and it will plot an histogram for every column. Histograms will overlap themselves and can be interactively activated or deactivated by clicking on the graph legend.\n",
    "    df.iplot(kind = 'hist', bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.6 Spread plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When passing two dataframes columns we will get a line plot and a subline plot which will show the spread between the two values. Very useful when analysing stock and market graphs.\n",
    "    df[['A', 'B']].iplot(kind = 'spread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.7 Bubble plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is a scatter plot where the size of the points changes according to a third factor:\n",
    "    df.iplot(kind = 'bubble', x='col1', y='col2', size='col3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.8 Scatter matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For dataframes with numerical columns only, it will create a matrix containing scatter plots for all the column combinations and histograms for each column.\n",
    "    df.scatter_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.9 Candle plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Very useful for financial data, given a dataframe with highest, lowest, opening and closing prices for each of the days, it will create a candle plot where the upper and lower limits of the candle body are the opening and closing prices and where the upper and lower fuses are the highest and lowest intraday values.\n",
    "    df.iplot(kind = 'candle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEOGRAPHICAL PLOTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This focuses on geographical plots using plotly, altough matplotlib also has a basemap extension that allows for geographical static plots, nevertheless plotly has more interactive capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly can be a little difficult sometimes, here is a documentation cheat sheet: https://images.plot.ly/plotly-documentation/images/python_cheat_sheet.pdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Necessary imports:\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "As we are going to use plotly offline (there is an online version in order to save an store visualization but it is premium, the offline version is opensource) we need to install:\n",
    " \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For this kind of plots we need 2 variables: data and layout, specified below:\n",
    "\n",
    "data = dict(type = 'choropleth',\n",
    "            locations = ['AZ', 'CA', 'NY'],\n",
    "            locationmode = 'USA-states',\n",
    "            colorscale = 'Portland',\n",
    "            text = ['Arizona', 'California', 'New York'],\n",
    "            marker = dict(line = dict(color = 'rgb(12,12,12)', width = 1)),\n",
    "            z = [1, 2, 3],\n",
    "            colorbar = {'title':'Colorbar Title'})\n",
    "            \n",
    "where 'locations' makes reference to the USA state codes and 'locationmode' references the actual kind of map in the plotly database. 'colorscale' is a plot argument referencing the color map used, 'text' will contain labels of the plot, 'marker' will define the border line color and width, 'z' will contain the values to be represented and finally 'colorbar' is the title of the value representation.\n",
    "\n",
    "We can plot the world by passing the country codes as locations.\n",
    "\n",
    "layout = dict(geo = {'scope':'usa'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, once we have defined our variables we can call the plotting functions:\n",
    "    choromap = go.Figure(data = [data], layout = layout)\n",
    "    iplot(choromap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
